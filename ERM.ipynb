{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4eee385-36a7-4200-86ad-4404ce7fc0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Empirical Risk: 5.997800192536225\n",
      "Iteration 100, Empirical Risk: 0.27687754673180687\n",
      "Iteration 200, Empirical Risk: 0.21536728196419652\n",
      "Iteration 300, Empirical Risk: 0.2146004869876152\n",
      "Iteration 400, Empirical Risk: 0.2145876318735173\n",
      "Iteration 500, Empirical Risk: 0.2145873108459609\n",
      "Iteration 600, Empirical Risk: 0.21458729983355457\n",
      "Iteration 700, Empirical Risk: 0.21458729938647395\n",
      "Iteration 800, Empirical Risk: 0.21458729936689303\n",
      "Iteration 900, Empirical Risk: 0.21458729936600573\n",
      "Learned weights: [ 1.47897143 -2.02529826  1.02072218]\n",
      "Learned bias: 0.4089768375447561\n",
      "Final Empirical Risk: 0.21458729936596502\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def empirical_risk(w, b, X, y):\n",
    "    n = X.shape[0]\n",
    "    predictions = X.dot(w) + b\n",
    "    residuals = y - predictions\n",
    "    mse = np.mean(residuals ** 2)\n",
    "    return mse\n",
    "\n",
    "def gradient_descent(X, y, C, learning_rate=0.01, num_iterations=1000):\n",
    "    n, d = X.shape\n",
    "    w = np.random.randn(d)\n",
    "    b = 0.0\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        predictions = X.dot(w) + b\n",
    "        residuals = y - predictions\n",
    "        \n",
    "        w_grad = -2/n * X.T.dot(residuals)\n",
    "        b_grad = -2/n * np.sum(residuals)\n",
    "        \n",
    "        # Update weights with gradient descent\n",
    "        w -= learning_rate * w_grad\n",
    "        b -= learning_rate * b_grad\n",
    "\n",
    "        # Apply the constraint on the weights\n",
    "        if np.linalg.norm(w) > C:\n",
    "            w = w / np.linalg.norm(w) * C\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Iteration {i}, Empirical Risk: {empirical_risk(w, b, X, y)}')\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(100, 3)\n",
    "    true_w = np.array([1.5, -2.0, 1.0])\n",
    "    true_b = 0.5\n",
    "    y = X.dot(true_w) + true_b + 0.5 * np.random.randn(100)\n",
    "\n",
    "    # Define the bound for the weight vector\n",
    "    C = 5.0\n",
    "\n",
    "    # Perform ERM with bounded linear function class\n",
    "    w, b = gradient_descent(X, y, C)\n",
    "\n",
    "    print(f'Learned weights: {w}')\n",
    "    print(f'Learned bias: {b}')\n",
    "    print(f'Final Empirical Risk: {empirical_risk(w, b, X, y)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c41841-e71e-4a68-8bd9-bb014f85110d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
